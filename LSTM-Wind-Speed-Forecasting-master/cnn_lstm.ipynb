{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/observations/081810-99999-2013.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-96af94214afb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0myears\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2013\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2018\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_observations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0moriginal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-96af94214afb>\u001b[0m in \u001b[0;36mread_observations\u001b[1;34m(years, usaf, wban)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myears\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/observations/{usaf}-{wban}-{year}.gz\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0musaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwban\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwban\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgz\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\voghoei\\AppData\\Local\\Continuum\\anaconda3\\envs\\py35\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\voghoei\\AppData\\Local\\Continuum\\anaconda3\\envs\\py35\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/observations/081810-99999-2013.gz'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "from io import BytesIO\n",
    "from ish_parser import ish_parser\n",
    "\n",
    "def read_observations(years, usaf='081810', wban='99999'):\n",
    "    parser = ish_parser()\n",
    "    \n",
    "    for year in years:\n",
    "        path = \"../data/observations/{usaf}-{wban}-{year}.gz\".format(year=year, usaf=usaf, wban=wban)\n",
    "        with gzip.open(path) as gz:\n",
    "            parser.loads(bytes.decode(gz.read()))\n",
    "            \n",
    "    reports = parser.get_reports()\n",
    "    \n",
    "    station_latitudes = [41.283, 41.293]\n",
    "    # station_latitudes = [40.080, 40.090]\n",
    "    observations = pd.DataFrame.from_records(((r.datetime, \n",
    "                                               r.air_temperature.get_numeric(),\n",
    "                                               (r.precipitation[0]['depth'].get_numeric() if r.precipitation else 0),\n",
    "                                               r.humidity.get_numeric(),\n",
    "                                               r.sea_level_pressure.get_numeric(),\n",
    "                                               r.wind_speed.get_numeric(),\n",
    "                                               r.wind_direction.get_numeric()) \n",
    "                                              for r in reports if r.latitude in station_latitudes and r.datetime.minute == 0),\n",
    "                             columns=['timestamp', 'AT', 'precipitation', 'humidity', 'pressure', 'wind_speed', 'wind_direction'], \n",
    "                             # columns=['timestamp', 'AT', 'precipitation', 'humidity', 'wind_speed', 'wind_direction'],\n",
    "                             index='timestamp')\n",
    "    \n",
    "    return observations\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "years = range(2013, 2018)\n",
    "dataset = read_observations(years)\n",
    "\n",
    "original = dataset.copy(deep=True)\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "np.random.seed(1234)\n",
    "\n",
    "def drop_duplicates(df):\n",
    "    print(\"Number of duplicates: {}\".format(len(df.index.get_duplicates())))\n",
    "    return df[~df.index.duplicated(keep='first')]\n",
    "    \n",
    "def impute_missing(df):\n",
    "    # todo test with moving average / mean or something smarter than forward fill\n",
    "    print(\"Number of rows with nan: {}\".format(np.count_nonzero(df.isnull())))\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    return df\n",
    "    \n",
    "def first_order_difference(data, columns):\n",
    "    for column in columns:\n",
    "        data[column+'_d'] = data[column].diff(periods=1)\n",
    "    \n",
    "    return data.dropna()\n",
    "\n",
    "# def derive_prediction_columns(data, column, column2, horizons):\n",
    "#     for look_ahead in horizons:\n",
    "#         data['prediction_' + str(look_ahead)] = data[column].diff(periods=look_ahead).shift(-look_ahead)\n",
    "        \n",
    "#     for look_ahead in horizons:\n",
    "#         data['prediction_direction_' + str(look_ahead)] = data[column2].diff(periods=look_ahead).shift(-look_ahead)\n",
    "\n",
    "def derive_prediction_columns(data, column, horizons):\n",
    "    for look_ahead in horizons:\n",
    "        data['prediction_' + str(look_ahead)] = data[column].diff(periods=look_ahead).shift(-look_ahead)\n",
    "    \n",
    "    return data.dropna()\n",
    "\n",
    "def scale_features(scaler, features):\n",
    "    # scaler.fit(features)\n",
    "    \n",
    "    scaled = scaler.fit_transform(features)\n",
    "    scaled = pd.DataFrame(scaled, columns=features.columns)\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def inverse_prediction_scale(scaler, predictions, original_columns, column):\n",
    "    loc = original_columns.get_loc(column)\n",
    "    \n",
    "    inverted = np.zeros((len(predictions), len(original_columns)))\n",
    "    inverted[:,loc] = np.reshape(predictions, (predictions.shape[0],))\n",
    "    \n",
    "    inverted = scaler.inverse_transform(inverted)[:,loc] # Scale back the data to the original representation\n",
    "    \n",
    "    return inverted\n",
    "\n",
    "# def invert_all_prediction_scaled(scaler, predictions, original_columns, horizons):\n",
    "#     inverted = np.zeros(predictions.shape)\n",
    "#     inverted2 = np.zeros(predictions.shape)\n",
    "    \n",
    "#     for col_idx, horizon in enumerate(horizons):\n",
    "#         inverted[:,col_idx] = inverse_prediction_scale(\n",
    "#             scaler, predictions[:,col_idx], \n",
    "#             original_columns,\n",
    "#             \"prediction_\" + str(horizon))\n",
    "\n",
    "#     for col_idx, horizon in enumerate(horizons):\n",
    "#         inverted2[:,col_idx] = inverse_prediction_scale(\n",
    "#             scaler, predictions[:,col_idx], \n",
    "#             original_columns,\n",
    "#             \"prediction_direction_\" + str(horizon))\n",
    "    \n",
    "#     return inverted, inverted2\n",
    "\n",
    "def invert_all_prediction_scaled(scaler, predictions, original_columns, horizons):\n",
    "    inverted = np.zeros(predictions.shape)\n",
    "    \n",
    "    for col_idx, horizon in enumerate(horizons):\n",
    "        inverted[:,col_idx] = inverse_prediction_scale(\n",
    "            scaler, predictions[:,col_idx], \n",
    "            original_columns,\n",
    "            \"prediction_\" + str(horizon))\n",
    "    \n",
    "    return inverted\n",
    "\n",
    "def inverse_prediction_difference(predictions, original):\n",
    "    return predictions + original\n",
    "\n",
    "def invert_all_prediction_differences(predictions, original):\n",
    "    inverted = predictions\n",
    "    \n",
    "    for col_idx, horizon in enumerate(horizons):\n",
    "        inverted[:, col_idx] = inverse_prediction_difference(predictions[:,col_idx], original)\n",
    "        \n",
    "    return inverted\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "dataset = drop_duplicates(dataset)\n",
    "dataset = impute_missing(dataset)\n",
    "dataset.describe()\n",
    "\n",
    "#select features we're going to use\n",
    "# features = dataset[['wind_speed', \n",
    "#                     'nems4_wind_speed',\n",
    "#                     'wind_direction',\n",
    "#                     'nems4_wind_direction', \n",
    "#                     'AT', \n",
    "#                     'nems4_AT', \n",
    "#                     'humidity', \n",
    "#                     'nems4_humidity',\n",
    "#                     'pressure',\n",
    "#                     'nems4_pressure']]\n",
    "\n",
    "features = dataset[['wind_speed', \n",
    "                    'wind_direction', \n",
    "                    'AT', \n",
    "                    'precipitation',\n",
    "                    'humidity',\n",
    "                    'pressure'\n",
    "                    ]]\n",
    "\n",
    "# the time horizons we're going to predict (in hours)\n",
    "horizons = [1, 2, 3, 4]\n",
    "\n",
    "features = first_order_difference(features, features.columns)\n",
    "features = derive_prediction_columns(features, 'wind_speed', horizons)\n",
    "# features = derive_prediction_columns(features, 'wind_speed', 'wind_direction', horizons)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "scaled = scale_features(scaler, features)\n",
    "\n",
    "scaled.describe()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def prepare_test_train(data, features, predictions, sequence_length, split_percent=0.9):\n",
    "    \n",
    "    num_features = len(features)\n",
    "    num_predictions = len(predictions)\n",
    "    \n",
    "    # make sure prediction cols are at end\n",
    "    columns = features + predictions\n",
    "    \n",
    "    data = data[columns].values\n",
    "    \n",
    "    print(\"Using {} features to predict {} horizons\".format(num_features, num_predictions))\n",
    "    \n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length+1):\n",
    "        result.append(data[index:index + sequence_length])\n",
    "\n",
    "    result = np.array(result)\n",
    "    # shape (n_samples, sequence_length, num_features + num_predictions)\n",
    "    print(\"Shape of data: {}\".format(np.shape(result)))\n",
    "    \n",
    "    row = round(split_percent * result.shape[0])\n",
    "    train = result[:row, :]\n",
    "    \n",
    "    X_train = train[:, :, :-num_predictions]\n",
    "    y_train = train[:, -1, -num_predictions:]\n",
    "    X_test = result[row:, :, :-num_predictions]\n",
    "    y_test = result[row:, -1, -num_predictions:]\n",
    "    \n",
    "    print(\"Shape of X train: {}\".format(np.shape(X_train)))\n",
    "    print(\"Shape of y train: {}\".format(np.shape(y_train)))\n",
    "    print(\"Shape of X test: {}\".format(np.shape(X_test)))\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], num_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], num_features))\n",
    "    \n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], num_predictions))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], num_predictions))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, row\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "sequence_length = 48\n",
    "\n",
    "prediction_cols = ['prediction_' + str(h) for h in horizons]\n",
    "# prediction_cols_2 = ['prediction_direction_' + str(h) for h in horizons]\n",
    "# prediction_cols = prediction_cols + prediction_cols_2\n",
    "\n",
    "# 多特征\n",
    "# feature_cols = ['wind_speed_d', 'nems4_wind_speed_d', \n",
    "#                 # 'wind_direction_d', 'nems4_wind_direction_d',\n",
    "#                 'AT_d', 'nems4_AT_d', \n",
    "#                 'humidity_d', 'nems4_humidity_d', \n",
    "#                 'pressure_d', 'nems4_pressure_d']\n",
    "\n",
    "# 少特征\n",
    "feature_cols = ['wind_speed_d', \n",
    "#                 'wind_direction_d', \n",
    "                'AT_d',\n",
    "                # 'precipitation_d', \n",
    "                'humidity_d',\n",
    "                'pressure_d'\n",
    "                ]\n",
    "\n",
    "# 不做一阶差分的特征项\n",
    "# feature_cols = ['wind_speed', \n",
    "# #                 'wind_direction', \n",
    "#                 'AT', \n",
    "#                 'humidity', \n",
    "#                 'pressure']\n",
    "\n",
    "X_train, y_train, X_test, y_test, row_split = prepare_test_train(\n",
    "    scaled,\n",
    "    feature_cols,\n",
    "    prediction_cols,\n",
    "    sequence_length,\n",
    "    split_percent = 0.9)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#(-1 is because we only take the last y row in each sequence)\n",
    "sequence_offset = sequence_length - 1\n",
    "\n",
    "# validate train\n",
    "inverse_scale = invert_all_prediction_scaled(scaler, y_train, scaled.columns, horizons)\n",
    "\n",
    "assert(mean_squared_error(\n",
    "    features[prediction_cols][sequence_offset:row_split+sequence_offset], \n",
    "    inverse_scale) < 1e-10)\n",
    "\n",
    "\n",
    "undiff_prediction = invert_all_prediction_differences(\n",
    "    inverse_scale, \n",
    "    features['wind_speed'][sequence_offset:row_split+sequence_offset])\n",
    "\n",
    "for i, horizon in enumerate(horizons):\n",
    "    assert(mean_squared_error(\n",
    "        features['wind_speed'][sequence_offset+horizon:row_split+sequence_offset+horizon], \n",
    "        undiff_prediction[:,i]) < 1e-10)\n",
    "\n",
    "    \n",
    "# validate test\n",
    "inverse_scale = invert_all_prediction_scaled(scaler, y_test, scaled.columns, horizons)\n",
    "\n",
    "assert(mean_squared_error(\n",
    "    features[prediction_cols][sequence_offset+row_split:], \n",
    "    inverse_scale) < 1e-10)\n",
    "\n",
    "undiff_prediction = invert_all_prediction_differences(\n",
    "    inverse_scale, \n",
    "    features['wind_speed'][sequence_offset+row_split:])\n",
    "\n",
    "for i, horizon in enumerate(horizons):\n",
    "    assert(mean_squared_error(\n",
    "        features['wind_speed'][sequence_offset+row_split+horizon:], \n",
    "        undiff_prediction[:-horizon,i]) < 1e-10)\n",
    "\n",
    "\n",
    "filename = os.path.basename(__file__)\n",
    "batch_size = 512\n",
    "epochs = 300\n",
    "\n",
    "# Build the LSTM Model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from keras.utils import plot_model\n",
    "from keras import losses\n",
    "\n",
    "\n",
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(128, 3, \n",
    "                    padding='same', \n",
    "                    activation='relu', \n",
    "                    input_shape=(sequence_length, layers[0])))\n",
    "\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv1D(64, 3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "    model.add(LSTM(16))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(layers[1]))\n",
    "    model.add(Activation('linear'))\n",
    "    \n",
    "    model.compile(loss='mae', optimizer='rmsprop')\n",
    "    \n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='./model/model_'+filename+'.png', show_shapes=True)\n",
    "          \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_network(X_train, y_train, X_test, y_test, layers, epochs, batch_size=batch_size):\n",
    "    model = build_model(layers)\n",
    "    history = None\n",
    "    \n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs,\n",
    "            validation_split=0.1,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir='./logs', write_graph=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=60, \n",
    "                                verbose=1, mode='auto', min_lr=0.0001),\n",
    "                EarlyStopping(monitor='val_loss', patience=80, verbose=1, mode='auto'),\n",
    "                ModelCheckpoint('./model/best_'+filename+'.hdf5', monitor='val_loss', verbose=1, \n",
    "                                save_best_only=True, mode='auto')\n",
    "            ])\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted\")\n",
    "    \n",
    "    predicted = model.predict(X_test)\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return model, predicted, history, scores\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "model, predicted, history, scores = run_network(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test,\n",
    "    y_test,\n",
    "    layers = [X_train.shape[2], y_train.shape[1]],\n",
    "    epochs=epochs)\n",
    "\n",
    "\n",
    "print(scores)\n",
    "print(history.history.keys())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# loss\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig('./image/loss_' + filename + '.png')\n",
    "plt.show()\n",
    "\n",
    "# print(type(predicted)) # <class 'numpy.ndarray'>\n",
    "# print(predicted.shape) # (7594, 6)\n",
    "print(\"*********************************************************\")\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "print(\"MAE {:.3}, RMSE {:.3f}\".format(\n",
    "    mean_absolute_error(y_test, predicted),\n",
    "    sqrt(mean_squared_error(y_test, predicted))))\n",
    "\n",
    "# for i, horizon in enumerate(horizons):\n",
    "#     print(\"wind speed: MAE {:.3f}, RMSE {:.3f} for horizon {}\".format(\n",
    "#         mean_absolute_error(y_test[:,i], predicted[:,i]),\n",
    "#         sqrt(mean_squared_error(y_test[:,i], predicted[:,i])),\n",
    "#         horizon))\n",
    "\n",
    "# for i, horizon in enumerate(horizons):\n",
    "#     print(\"wind direction: MAE {:.3f}, RMSE {:.3f} for horizon {}\".format(\n",
    "#         mean_absolute_error(y_test[:,i], predicted[:,i]),\n",
    "#         sqrt(mean_squared_error(y_test[:,i], predicted[:,i])),\n",
    "#         horizon))\n",
    "\n",
    "\n",
    "sequence_offset = sequence_length - 1\n",
    "\n",
    "# inverse_scale, inverse_scale2 = invert_all_prediction_scaled(scaler, predicted, scaled.columns, horizons)\n",
    "inverse_scale = invert_all_prediction_scaled(scaler, predicted, scaled.columns, horizons)\n",
    "# print(inverse_scale.shape) # (7594, 6)\n",
    "# print(\"*********************************************************************\")\n",
    "# print(inverse_scale2.shape) # (7594, 6)\n",
    "\n",
    "predicted_signal = invert_all_prediction_differences(\n",
    "    inverse_scale, \n",
    "    features['wind_speed'][sequence_offset+row_split:])\n",
    "\n",
    "# print(predicted_signal[:5])\n",
    "# print(\"Shape of predicted_signal:\", predicted_signal.shape) # (2588, 3)\n",
    "\n",
    "# predicted_signal2 = invert_all_prediction_differences(\n",
    "#     inverse_scale2, \n",
    "#     features['wind_direction'][sequence_offset+row_split:])\n",
    "# print(predicted_signal2[:5])\n",
    "# print(\"***************************************************************************\")\n",
    "\n",
    "from sklearn.utils.validation import check_array\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true = check_array(y_true, ensure_2d=False)\n",
    "    y_pred = check_array(y_pred, ensure_2d=False)\n",
    "\n",
    "    return np.mean(np.abs((y_pred - y_true) / y_pred)) * 100\n",
    "\n",
    "for i, horizon in enumerate(horizons):\n",
    "    a = features['wind_speed'][sequence_offset+row_split+horizon:]\n",
    "    p = predicted_signal[:-horizon, i]\n",
    "    # print(\"Shape of a:\", a.shape)  # (7591,) (7590,) (7589,) (7588,)\n",
    "    # print(\"Shape of p:\", p.shape)  # (7591,) (7590,) (7589,) (7588,)\n",
    "    mae = mean_absolute_error(a, p)\n",
    "    rmse = sqrt(mean_squared_error(a, p))\n",
    "    mape = np.average(np.abs((a-p)/p))*100\n",
    "    mape2 = mean_absolute_percentage_error(a, p)\n",
    "    nrmse_mean = 100*rmse/(a.mean())\n",
    "    \n",
    "    print(\"Real scale wind speed: MAE {:.3f}, RMSE {:.3f}, MAPE {:.3f}, MAPE2 {:.3f}, NRMSE {:.3f} for horizon {}\".format(\n",
    "            mae, rmse, mape, mape2, nrmse_mean, horizon))\n",
    "    \n",
    "# for i, horizon in enumerate(horizons):\n",
    "#     a2 = features['wind_direction'][sequence_offset+row_split+horizon:]\n",
    "#     p2 = predicted_signal2[:-horizon, i]\n",
    "\n",
    "#     print(\"Real scale wind direction: MAE {:.3f}, RMSE {:.3f} for horizon {}\".format(\n",
    "#             mean_absolute_error(a2, p2),\n",
    "#             sqrt(mean_squared_error(a2, p2)),\n",
    "#             horizon))\n",
    "\n",
    "\n",
    "plot_samples=500\n",
    "max_horizon = horizons[-1]\n",
    "plots = len(horizons)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5 * plots))\n",
    "fig.suptitle(\"Model Prediction at each Horizon\")\n",
    "\n",
    "for i, horizon in enumerate(horizons):\n",
    "    plt.subplot(plots, 1, i+1)\n",
    "    \n",
    "    len_adjust = max_horizon-horizon # ensure all have same lenght\n",
    "    \n",
    "    real = features['wind_speed'][sequence_offset+row_split+horizon+len_adjust:].values\n",
    "    pred = predicted_signal[len_adjust:-horizon,i]\n",
    "    \n",
    "    plt.plot(real[:plot_samples], label='observed')\n",
    "    plt.plot(pred[:plot_samples], label='predicted')\n",
    "    plt.title(\"Prediction for {} Hour Horizon\".format(horizon))\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Wind Speed (m/s)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.savefig('./image/result_' + filename + '.png')\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
