{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.datasets import imdb\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label content\n",
       "0  Bachelors Degree       A\n",
       "1  Bachelors Degree       B\n",
       "2  Bachelors Degree       S"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "df = pd.read_csv('sample.csv',encoding='latin-1', header=0,  sep = ',', names = ['label', 'content'], error_bad_lines=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label content\n",
       "0  Bachelors Degree       A\n",
       "1  Bachelors Degree       B\n",
       "2  Bachelors Degree       S"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning\n",
    "\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "#df= df.dropna()\n",
    "df[df[\"label\"].apply(lambda x: x.isnumeric())] #dropping null values\n",
    "df[df.label.apply(lambda x: x !=\"\")] #filtering out rows with non-numeric characters in the \"label\" column\n",
    "df[df.content.apply(lambda x: x !=\"\")] #filterin out rows with empty comments\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words= 66\n",
      "maxlen of all sentences= 3\n",
      "[[2], [4], [6], [9], [15], [16], [17], [18], [19], [20], [21], [9, 10], [22], [23], [24], [25], [26], [27], [28], [29], [7, 30], [31], [7, 11], [32], [33], [34], [3, 35], [3, 36], [3, 37], [3, 38], [3, 39], [1, 2, 8], [1, 2, 6], [1, 2, 40], [1, 41], [1, 42], [1, 4, 2], [1, 4, 8], [1, 4, 11], [1, 12, 8], [1, 12, 10], [1, 43, 6], [13, 44], [13, 45], [46, 47], [48, 49], [50, 14], [7, 51], [5, 52], [5, 53], [5, 54], [5, 55], [56, 14], [57], [58], [59], [60, 61], [62], [63], [64, 65]]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize with padding \n",
    "\n",
    "def get_sequence_of_tokens(x):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Encode training data sentences into sequences\n",
    "    input_sequences = tokenizer.texts_to_sequences(x)    \n",
    "    return input_sequences, total_words\n",
    "\n",
    "\n",
    "sequences,num_words  = get_sequence_of_tokens(df['content'])\n",
    "\n",
    "print(\"num_words= \"+str(num_words))\n",
    "print(\"maxlen of all sentences= \"+ str(maxlen))\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  2]\n",
      " [ 0  0  4]\n",
      " [ 0  0  6]\n",
      " [ 0  0  9]\n",
      " [ 0  0 15]\n",
      " [ 0  0 16]\n",
      " [ 0  0 17]\n",
      " [ 0  0 18]\n",
      " [ 0  0 19]\n",
      " [ 0  0 20]\n",
      " [ 0  0 21]\n",
      " [ 0  9 10]\n",
      " [ 0  0 22]\n",
      " [ 0  0 23]\n",
      " [ 0  0 24]\n",
      " [ 0  0 25]\n",
      " [ 0  0 26]\n",
      " [ 0  0 27]\n",
      " [ 0  0 28]\n",
      " [ 0  0 29]\n",
      " [ 0  7 30]\n",
      " [ 0  0 31]\n",
      " [ 0  7 11]\n",
      " [ 0  0 32]\n",
      " [ 0  0 33]\n",
      " [ 0  0 34]\n",
      " [ 0  3 35]\n",
      " [ 0  3 36]\n",
      " [ 0  3 37]\n",
      " [ 0  3 38]\n",
      " [ 0  3 39]\n",
      " [ 1  2  8]\n",
      " [ 1  2  6]\n",
      " [ 1  2 40]\n",
      " [ 0  1 41]\n",
      " [ 0  1 42]\n",
      " [ 1  4  2]\n",
      " [ 1  4  8]\n",
      " [ 1  4 11]\n",
      " [ 1 12  8]\n",
      " [ 1 12 10]\n",
      " [ 1 43  6]\n",
      " [ 0 13 44]\n",
      " [ 0 13 45]\n",
      " [ 0 46 47]\n",
      " [ 0 48 49]\n",
      " [ 0 50 14]\n",
      " [ 0  7 51]\n",
      " [ 0  5 52]\n",
      " [ 0  5 53]\n",
      " [ 0  5 54]\n",
      " [ 0  5 55]\n",
      " [ 0 56 14]\n",
      " [ 0  0 57]\n",
      " [ 0  0 58]\n",
      " [ 0  0 59]\n",
      " [ 0 60 61]\n",
      " [ 0  0 62]\n",
      " [ 0  0 63]\n",
      " [ 0 64 65]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Padding    lenght(pad) is the max number or word in the sentences and each number is one of the word\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors = input_sequences\n",
    "    return predictors, max_sequence_len\n",
    "\n",
    "data, m = generate_padded_sequences(sequences)\n",
    "print(data)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#matrix length(pad) is the number of vocab and each 1 is represent if that word is appear in the sentence or not\n",
    "encoded_docs = tokenizer.sequences_to_matrix(sequences, mode='binary')\n",
    "print(encoded_docs[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 100, input_length=50)) \n",
    "### an embedding layer to expand tokens, allowing NN to represent the word in a meaningful way ###\n",
    "######Embedding(vocabolary size, dimension of embedding, MAX length of each sentence)####\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x17c70378048>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputSize = 100\n",
    "e = Embedding(num_words, outputSize, input_length=maxlen)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
