{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label content\n",
       "0  Bachelors Degree       A\n",
       "1  Bachelors Degree       B\n",
       "2  Bachelors Degree       S"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "df = pd.read_csv('sample.csv',encoding='latin-1', header=0,  sep = ',', names = ['label', 'content'], error_bad_lines=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bachelors Degree</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label content\n",
       "0  Bachelors Degree       A\n",
       "1  Bachelors Degree       B\n",
       "2  Bachelors Degree       S"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cleaning\n",
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "#df= df.dropna()\n",
    "df[df[\"label\"].apply(lambda x: x.isnumeric())] #dropping null values\n",
    "df[df.label.apply(lambda x: x !=\"\")] #filtering out rows with non-numeric characters in the \"label\" column\n",
    "df[df.content.apply(lambda x: x !=\"\")] #filterin out rows with empty comments\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             a\n",
       "1             b\n",
       "2             s\n",
       "3            bs\n",
       "4            bt\n",
       "5            bw\n",
       "6            cs\n",
       "7            dm\n",
       "8           ene\n",
       "9           zbm\n",
       "10          zbt\n",
       "11         bs/p\n",
       "12         bsac\n",
       "13         bsad\n",
       "14         bsfs\n",
       "15        aasts\n",
       "16        aasvt\n",
       "17        aaswe\n",
       "18        aauct\n",
       "19        appit\n",
       "20        as.ds\n",
       "21        asche\n",
       "22        as-ed\n",
       "23        aslas\n",
       "24        asmlt\n",
       "25        assoc\n",
       "26       bsb/pj\n",
       "27       bsb/pm\n",
       "28       bsb/ps\n",
       "29       bsb/rf\n",
       "30       bsb/rm\n",
       "31       m.a.e.\n",
       "32       m.a.s.\n",
       "33       m.a.t.\n",
       "34       m.acc.\n",
       "35       m.arch\n",
       "36       m.b.a.\n",
       "37       m.b.e.\n",
       "38       m.b.ed\n",
       "39       m.c.e.\n",
       "40       m.c.p.\n",
       "41       m.d.s.\n",
       "42      aati&la\n",
       "43      aati&tm\n",
       "44      aatm&nt\n",
       "45      aats&et\n",
       "46      adn/aas\n",
       "47      as.math\n",
       "48      ata cis\n",
       "49      ata cst\n",
       "50      ata ece\n",
       "51      ata obt\n",
       "52      eet.aas\n",
       "53      english\n",
       "54      geology\n",
       "55      history\n",
       "56      non mat\n",
       "57      physics\n",
       "58      spanish\n",
       "59    elder law\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_stop_words(sentences, stop_words):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sent = [word for word in sentence.split() if word not in stop_words]\n",
    "        sentences[i] = ' '.join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))-set(['a','A','i','I','t','T','as','AS','S','s'])\n",
    "df['content'] = filter_stop_words(df['content'].str.lower(), stop_words)\n",
    "df['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization\n",
    "\n",
    "## In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a languageâ€™s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "### https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
    "\n",
    "# it is removing s at the end as the plurar sign!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             a\n",
       "1             b\n",
       "2             s\n",
       "3             b\n",
       "4            bt\n",
       "5            bw\n",
       "6             c\n",
       "7            dm\n",
       "8           ene\n",
       "9           zbm\n",
       "10          zbt\n",
       "11         bs/p\n",
       "12         bsac\n",
       "13         bsad\n",
       "14         bsfs\n",
       "15        aasts\n",
       "16        aasvt\n",
       "17        aaswe\n",
       "18        aauct\n",
       "19        appit\n",
       "20        as.ds\n",
       "21        asche\n",
       "22        as-ed\n",
       "23        aslas\n",
       "24        asmlt\n",
       "25        assoc\n",
       "26       bsb/pj\n",
       "27       bsb/pm\n",
       "28       bsb/ps\n",
       "29       bsb/rf\n",
       "30       bsb/rm\n",
       "31       m.a.e.\n",
       "32       m.a.s.\n",
       "33       m.a.t.\n",
       "34       m.acc.\n",
       "35       m.arch\n",
       "36       m.b.a.\n",
       "37       m.b.e.\n",
       "38       m.b.ed\n",
       "39       m.c.e.\n",
       "40       m.c.p.\n",
       "41       m.d.s.\n",
       "42      aati&la\n",
       "43      aati&tm\n",
       "44      aatm&nt\n",
       "45      aats&et\n",
       "46      adn/aas\n",
       "47      as.math\n",
       "48       ata ci\n",
       "49      ata cst\n",
       "50      ata ece\n",
       "51      ata obt\n",
       "52      eet.aas\n",
       "53      english\n",
       "54      geology\n",
       "55      history\n",
       "56      non mat\n",
       "57       physic\n",
       "58      spanish\n",
       "59    elder law\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the steming libraries to be used\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# initialise the lemmatizer\n",
    "\n",
    "\n",
    "def lemmatize(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sent = [lemmatizer.lemmatize(w) for w in sentence.split()]\n",
    "        sentences[i] = ' '.join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# apply the lemmatizer function on the content column\n",
    "\n",
    "df['content'] = lemmatize(df['content'].str.lower())\n",
    "df['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization is better than Stemming\n",
    "\n",
    "## check https://www.guru99.com/stemming-lemmatization-python-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             a\n",
       "1             b\n",
       "2             s\n",
       "3            bs\n",
       "4            bt\n",
       "5            bw\n",
       "6            cs\n",
       "7            dm\n",
       "8           ene\n",
       "9           zbm\n",
       "10          zbt\n",
       "11         bs/p\n",
       "12         bsac\n",
       "13         bsad\n",
       "14          bsf\n",
       "15         aast\n",
       "16        aasvt\n",
       "17         aasw\n",
       "18        aauct\n",
       "19        appit\n",
       "20         as.d\n",
       "21         asch\n",
       "22          as-\n",
       "23         asla\n",
       "24        asmlt\n",
       "25        assoc\n",
       "26       bsb/pj\n",
       "27       bsb/pm\n",
       "28        bsb/p\n",
       "29       bsb/rf\n",
       "30       bsb/rm\n",
       "31       m.a.e.\n",
       "32       m.a.s.\n",
       "33       m.a.t.\n",
       "34       m.acc.\n",
       "35       m.arch\n",
       "36       m.b.a.\n",
       "37       m.b.e.\n",
       "38       m.b.ed\n",
       "39       m.c.e.\n",
       "40       m.c.p.\n",
       "41       m.d.s.\n",
       "42      aati&la\n",
       "43      aati&tm\n",
       "44      aatm&nt\n",
       "45      aats&et\n",
       "46       adn/aa\n",
       "47      as.math\n",
       "48       ata ci\n",
       "49      ata cst\n",
       "50      ata ece\n",
       "51      ata obt\n",
       "52       eet.aa\n",
       "53      english\n",
       "54       geolog\n",
       "55      histori\n",
       "56      non mat\n",
       "57       physic\n",
       "58      spanish\n",
       "59    elder law\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming is a method of normalization of words in Natural Language Processing. It is a technique in which a set of words in a sentence are converted into a sequence to shorten its lookup. In this method, the words having the same meaning but have some variations according to the context or sentence are normalized.\n",
    "# https://www.guru99.com/stemming-lemmatization-python-nltk.html\n",
    "\n",
    "def portStemmer(sentences):\n",
    "    porter = PorterStemmer()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        new_sent = [porter.stem(w) for w in sentence.split()]\n",
    "        sentences[i] = ' '.join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# apply the Stemming function on the content column\n",
    "\n",
    "df['content'] = portStemmer(df['content'].str.lower())\n",
    "df['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize - integer encode - Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(docs):   \n",
    "\n",
    "    # prepare tokenizer\n",
    "    t = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', \n",
    "                  char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "    ## tokenize it to words. \n",
    "    t.fit_on_texts(docs)\n",
    "    \n",
    "    # Get our data word index\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    print('vocab_size: '+str(vocab_size))\n",
    "\n",
    "    # integer encode the documents.   \n",
    "    encoded_docs = t.texts_to_sequences(docs)\n",
    "    print(encoded_docs)\n",
    "\n",
    "    # Get max training sequence length\n",
    "    max_length = max([len(x) for x in encoded_docs])\n",
    "    print('max_length: '+str(max_length))\n",
    "\n",
    "    # Pad the training sequences\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post', truncating='post')\n",
    "    print(padded_docs)\n",
    "    \n",
    "    return padded_docs, max_length, vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 15\n",
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "max_length: 4\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "padded_docs, max_length, vocab_size= get_sequence_of_tokens(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "vocab_size: 66\n",
      "[[2], [4], [6], [9], [15], [16], [17], [18], [19], [20], [21], [9, 10], [22], [23], [24], [25], [26], [27], [28], [29], [7, 30], [31], [7, 11], [32], [33], [34], [3, 35], [3, 36], [3, 37], [3, 38], [3, 39], [1, 2, 8], [1, 2, 6], [1, 2, 40], [1, 41], [1, 42], [1, 4, 2], [1, 4, 8], [1, 4, 11], [1, 12, 8], [1, 12, 10], [1, 43, 6], [13, 44], [13, 45], [46, 47], [48, 49], [50, 14], [7, 51], [5, 52], [5, 53], [5, 54], [5, 55], [56, 14], [57], [58], [59], [60, 61], [62], [63], [64, 65]]\n",
      "max_length: 3\n",
      "[[ 2  0  0]\n",
      " [ 4  0  0]\n",
      " [ 6  0  0]\n",
      " [ 9  0  0]\n",
      " [15  0  0]\n",
      " [16  0  0]\n",
      " [17  0  0]\n",
      " [18  0  0]\n",
      " [19  0  0]\n",
      " [20  0  0]\n",
      " [21  0  0]\n",
      " [ 9 10  0]\n",
      " [22  0  0]\n",
      " [23  0  0]\n",
      " [24  0  0]\n",
      " [25  0  0]\n",
      " [26  0  0]\n",
      " [27  0  0]\n",
      " [28  0  0]\n",
      " [29  0  0]\n",
      " [ 7 30  0]\n",
      " [31  0  0]\n",
      " [ 7 11  0]\n",
      " [32  0  0]\n",
      " [33  0  0]\n",
      " [34  0  0]\n",
      " [ 3 35  0]\n",
      " [ 3 36  0]\n",
      " [ 3 37  0]\n",
      " [ 3 38  0]\n",
      " [ 3 39  0]\n",
      " [ 1  2  8]\n",
      " [ 1  2  6]\n",
      " [ 1  2 40]\n",
      " [ 1 41  0]\n",
      " [ 1 42  0]\n",
      " [ 1  4  2]\n",
      " [ 1  4  8]\n",
      " [ 1  4 11]\n",
      " [ 1 12  8]\n",
      " [ 1 12 10]\n",
      " [ 1 43  6]\n",
      " [13 44  0]\n",
      " [13 45  0]\n",
      " [46 47  0]\n",
      " [48 49  0]\n",
      " [50 14  0]\n",
      " [ 7 51  0]\n",
      " [ 5 52  0]\n",
      " [ 5 53  0]\n",
      " [ 5 54  0]\n",
      " [ 5 55  0]\n",
      " [56 14  0]\n",
      " [57  0  0]\n",
      " [58  0  0]\n",
      " [59  0  0]\n",
      " [60 61  0]\n",
      " [62  0  0]\n",
      " [63  0  0]\n",
      " [64 65  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_docs, max_length, vocab_size= get_sequence_of_tokens(df['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the whole embedding pretrain word dict into memory - glove.6B.100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x26788095b08>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open(r'C:\\Users\\voghoei\\Python\\glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model with embeding - creade more dimention of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4, 32)             9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 64,433\n",
      "Trainable params: 62,933\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "model.add(e)\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using text to matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_tokenSequence(doc):\n",
    "    # prepare tokenizer\n",
    "    Tok = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "    ## tokenize it to words. \n",
    "    Tok.fit_on_texts(docs)\n",
    "\n",
    "    # Get our data word index\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    print('vocab_size: '+str(vocab_size))\n",
    "\n",
    "    # integer encode the documents.   \n",
    "    encoded_docs = Tok.texts_to_sequences(docs)\n",
    "    print(encoded_docs)\n",
    "    \n",
    "    #matrix length(pad) is the number of vocab and each 1 is represent if that word is appear in the sentence or not\n",
    "\n",
    "    sequence_matrix_encode = Tok.sequences_to_matrix(encoded_docs, mode='binary')\n",
    "    print(sequence_matrix_encode[1:4])\n",
    "    \n",
    "    return sequence_matrix_encode, max_length, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 15\n",
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sequence_matrix_encode, max_length, vocab_size = get_matrix_of_tokenSequence(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare to one_hot \n",
    "### we can not count the vocabulary size as we process sentence by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 8], [7, 6], [13, 13], [1, 6], [7], [7], [7, 13], [1, 7], [7, 6], [8, 5, 8, 7]]\n",
      "[[ 2  8  0  0]\n",
      " [ 7  6  0  0]\n",
      " [13 13  0  0]\n",
      " [ 1  6  0  0]\n",
      " [ 7  0  0  0]\n",
      " [ 7  0  0  0]\n",
      " [ 7 13  0  0]\n",
      " [ 1  7  0  0]\n",
      " [ 7  6  0  0]\n",
      " [ 8  5  8  7]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 15   # our gusse or calculated before\n",
    "\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
